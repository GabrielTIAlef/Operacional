# Projeto Operacional
Este painel foi minha primeira demanda dentro da minha nova empresa, sou o primeiro analista de dados dentro da organização e como Junior foi um grande desafio levar e executar toda essa demanda sozinho.
A empresa demandou um projeto para gestão das tarefas diárias da empresa, para um melhor acompanhamento das atividades com uma visão mais critica e ponderada em meio a grande quantidade de tarefas, tendo me passado quais informações tem maior peso e necessidade de acompanhamento, com uma taxa de atualização quase como em streaming de forma automatizada e com o menor custo possível.
O projeto é alimentado com 2 diferentes fonte de dados, que são sites na Web, o Gestta e o Notion, sites que usam para gestão interna da empresa, aqui esta sendo iniciada toda essa parte de análise de dados, com isso, foi necessário a criação de todo workspace desde o começo, desde a criação de uma base de dados até ferramenta de BI, tendo que enxergar o melhor cenário para a melhor escolha dentro das necessidades da empresa.
Meu ETL escolhi Python dentro do Jupyter Notebook pois com as suas diversas bibliotecas facilita muitos passos e necessidades, dividi meu processo em 3 partes, criando um ambiente virtual para cada um deles evitando quebras e confusão de projetos e bibliotecas: 
1º = extração dos dados no Notion, ele disponibiliza uma API pública para extração de dados, com request crio uma conexão com a página e com a técnica do cursor consigo procurar e puxar os dados em especifico que preciso, pego esse dataframe e o exporto para CSV e JSON, tudo isso dentro de funções pois automatizei esse código para rodar a cada 1 hora com o agendador de tarefas.

2º = extração dos dados no Gestta, esse já não disponibiliza uma API pública, tive que usar uma técnica onde pego o endpoint exato que me traz os dados em que preciso, com isso crio todo um processo com request para conexão com a página e extraio os dados, criei um sistema de conexão com a ferramenta de mensagens que usamos que é o Slack para quando der algum erro, pois para essa API funcionar precisa de um token que é reinicializado todo dia, assim quando ele expirar é avisado e eu pego o novo token, transformo esse dataframe em CSV e JSON também, todo esse processo me retornando logs e dentro de funções pois automatizado ele roda a cada 7 minutos com o agendador de tarefas, dentro dele chamo um subprocess para chamar meu 3º ETL, garantindo que ele rode somente após o 2º.

3º = ETL final fazendo toda transformação e alimentação dos dados, com sqlalchemy eu crio uma conexão ao meu banco de dados, faço toda parte de transformação utilizando o Pandas o máximo que consigo porque assim evito muita carga dentro do banco e com o Pandas tem uma performance melhor, deixo tudo normalizado e padrão para melhor manipulação geral em todos diversos sistemas, exportando um arquivo em CSV para se necessário ter uma visualização em uma tabela mais rápida e pratica, tudo dentro de funções para automatizar todo o processo. 
Escolhi o PostgreSQL como ferramenta para base de dados, com o cenário identifiquei, não tem necessidade de passar a ser um banco em nuvem então o PostgreSQL é uma ferramenta que tem uma ótima capacidade para grande quantidade de dados, algo que dentro do cenário é necessário, e localmente funciona perfeitamente e de forma gratuita, criei duas tabelas sendo alimentadas pelo meu ETL e com eles criei uma view, para trazer um melhor desempenho no meu banco pois todas alterações consigo fazer diretamente nas minhas tabelas no ETL e deixo a visão final com todas alterações feitas para a view, com isso otimizando meu banco e também meu BI subindo somente a view e não o banco por inteiro.

Para ferramenta de BI, escolhi o Power BI, criando o painel com todas informações e necessidades que me foram passadas, com diversas medidas DAX como colunas calculadas, medidas rápidas, criando relações entre as tabelas, indicadores, entre outros, criando algumas regras de negócios e criando KPIS que me foram passadas, por conta da alta taxa de atualização do painel e foco em menor custo, usei uma técnica criando uma RPA, com Selenium, para automatizar a atualização do painel em 13 minutos, assim conseguindo entregar o que me foi pedido.
